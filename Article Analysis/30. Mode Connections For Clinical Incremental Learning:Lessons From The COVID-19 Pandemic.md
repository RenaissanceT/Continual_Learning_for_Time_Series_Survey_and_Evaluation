
# [Mode Connections For Clinical Incremental Learning: Lessons From The COVID-19 Pandemic](https://www.medrxiv.org/content/10.1101/2023.05.05.23289583v1.full)

----

<img width="1366" alt="Screenshot 2025-04-02 at 12 49 15 AM" src="https://github.com/user-attachments/assets/28eb95ac-e4af-476c-8005-c3f0ba0b8a7b" />

-----

### Domain-incremental learning， 2分类

----

### **领域** : 

增量学习，针对临床深度学习模型的适应性问题

### **分类** : 

域增量学习，研究如何应对因疾病演变和人口变化导致的数据分布变化

### **多任务** : 

实验部分使用了**COVID-19前后**的数据集进行分析，但未明确划分为多任务学习，而是强调模型在不同数据分布下的适应性

### **场景** : 

设定场景是**临床深度学习模型的域增量适应**，其目标是在疾病演变过程中保持模型的有效性。属于在线学习，因为模型需要在不同时间点不断调整学习策略

### **问题** : 

研究问题是**如何在临床环境中适应动态分布变化**，同时避免灾难性遗忘。域的划分基于**COVID-19前后**的数据变化，涉及不同疾病阶段的数据分布

### **方法** : 

提出一种**模式连接（Mode Connections）** 方法，该方法通过 **低损失路径（low-loss paths）** 连接神经网络的不同模式，以优化模型适应性。属于**预测**，因为目的是提高临床模型的稳定性和准确性。细分属于**Functional Regularization**，因为是一个通过学习低损失路径来减少灾难性遗忘，并优化模型迁移过程

### **为什么** : 

采用模式连接的原因是**传统增量学习方法在临床应用中容易导致性能下降**。通过学习低损失路径，该方法能够在不改变原始模型权重的情况下适应新数据分布，提高模型的长期稳定性


----

## **研究背景**

作者提出利用 **模式连接 mode connections**，即在神经网络参数空间中连接两个最小化点（模式）的低损失路径，来构建一个包含多个模式的网络。通过在现有模式和随机点之间学习一条低损失路径，并在该路径上找到适合新领域的模式，从而实现模型的增量更新，而无需修改原有模型的权重。该方法在牛津大学医院收集的COVID-19前后数据上进行了验证，展示了其在处理临床数据分布漂移方面的有效性。

![image](https://github.com/user-attachments/assets/423cec71-0a3a-41ef-a554-1aa4eeaf840b)

本文将增量或持续学习概念化为避免灾难性遗忘的增量模式网络, 拟议的框架为新域提供了一条可能的最小化路径。从这些低损耗路径中采样的模式可以用作集成，因此可以获得不确定性较小的预测。该方法通过在神经网络参数空间中连接两个最小化点（模式）的低损失路径，来构建一个包含多个模式的网络，从而实现模型的增量更新，而无需修改原有模型的权重。

## 算法（见上方图示）

1. **初始模式设定**：将当前已部署的模型参数设为模式 $\theta_A$。

2. **路径学习**：在新领域的数据上，学习从 $\theta_A$ 到一个随机点 $\theta_R$ 的低损失路径 $\gamma_{\theta_A \rightarrow \theta_R}$，该路径可通过贝塞尔曲线等方式建模。

![image](https://github.com/user-attachments/assets/b2e4cb6f-7ecb-42e8-a27c-a341196afd86)

3. **新模式确定**：在路径 $\gamma_{\theta_A \rightarrow \theta_R}$ 上，找到在新领域验证集上损失最小的点 $\theta_B$，作为新领域的模式。

![image](https://github.com/user-attachments/assets/40d916b3-3bfd-4524-ac71-f7f50e78b54f)

4. **模式网络扩展**：将路径 $\gamma_{\theta_A \rightarrow \theta_B}$ 添加到模式网络中，实现对新领域的增量学习。

5. **多领域扩展**：对于后续的新领域，重复上述步骤，逐步构建包含多个模式的网络。

## 模型架构与参数设置（Model Architecture and Parameter Setting）

DNN with 308 nodes → ReLU activation → Dropout with 0.25 rate → DNN with 231 nodes → ReLU activation → Dropout with 0.25 rate → DNN with 1 node → Sigmoid activation

- 模型使用 Adam 优化器进行训练，学习率固定为 $10^{-4}$，批次大小为 2048。
  
- 为实现 GDumb 和 GEM 方法，我们在每个新任务中保留来自先前领域的 5000 个样本（每类各 2500 个）作为缓冲区。

## 结论

本文提出了一种基于模式连接的增量学习方法。所提出的方法在学习新模式或领域时不会遭受灾难性的遗忘。OUH 数据的实验结果突出了所提出的方法的潜在好处。尽管本文提供了强有力的证据支持模式网络作为增量学习框架，但需要进行彻底的调查来分析这种方法在更具挑战性和多样化条件下的行为。需要更多的工作来降低所提出的方法的计算和空间复杂性。此外，我们没有强调用于降低预测不确定性的模式连接的集成方面。未来的工作将处理这些方面以改进所提出的方法。
