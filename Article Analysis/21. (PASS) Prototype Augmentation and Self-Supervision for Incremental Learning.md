# PASS / Prototype Augmentation - [Prototype Augmentation and Self-Supervision for Incremental Learning](https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Prototype_Augmentation_and_Self-Supervision_for_Incremental_Learning_CVPR_2021_paper.html)

----

<img width="1393" alt="Screen Shot 2024-10-16 at 12 22 19 PM" src="https://github.com/user-attachments/assets/3c634150-4266-404d-b84c-aa9a37d20459">

----

Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng- Lin Liu, “Prototype augmentation and self-supervision for in- cremental learning,” in Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, 2021, pp. 5871–5880.

**相关文献**：

Zhongzheng Qiao, Minghui Hu, Xudong Jiang, Ponnuthurai Nagaratnam Sugan-than, and Ramasamy Savitha. 2023. Class-Incremental Learning on Multivariate Time Series Via Shape-Aligned Temporal Distillation. In ICASSP. IEEE, 1–5. DT2W - [Class‐incremental learning on multivariate time series via shape‐aligned temporal distillation](https://dr.ntu.edu.sg/handle/10356/165392)


----

<img width="1436" alt="Screen Shot 2024-10-16 at 3 49 55 PM" src="https://github.com/user-attachments/assets/9575ab34-6d2b-47e0-85b1-ffc27d0a626f">

----

<img width="1418" alt="Screen Shot 2024-10-15 at 7 23 37 PM" src="https://github.com/user-attachments/assets/b40c8ef0-e334-498f-800f-827fe858cdaf">

---

# 核心要点

这篇论文的核心在于解决增量学习（Incremental Learning, IL）中的**灾难性遗忘**（Catastrophic Forgetting）问题。灾难性遗忘是指神经网络在学习新任务时会遗忘之前学到的知识，导致模型在旧任务上的表现大幅下降。为此，论文提出了一种新的非样本存储方法——**PASS**（Prototype Augmentation and Self-Supervision），它通过**原型增强**（Prototype Augmentation）和**自监督学习**（Self-Supervision）来保持旧任务的知识，提升模型在增量学习场景中的表现。

## PASS算法

PASS算法的核心思想是通过以下两个主要策略来应对灾难性遗忘和任务过拟合问题：

1. **原型增强（Prototype Augmentation）**：

在深度特征空间中，记忆每个旧类的代表性原型（即类中心的均值）。

当学习新任务时，对这些原型施加高斯噪声进行增强（扰动），生成多样化的特征表示。这些增强的原型与新数据的特征一起输入分类器，以保持旧类和新类之间的区分能力。

增强后的特征可以模拟旧类在特征空间中的潜在变化，减少学习新任务时对旧任务的影响。

2. **自监督学习（Self-Supervision, SSL）**：

通过自监督学习的方式，生成额外的代理任务来增强特征的泛化能力。论文采用**旋转变换**（例如90度、180度和270度）来生成新的类标签，使得模型可以在学习过程中捕捉到更泛化的特征表示。

这些自监督的任务与原始任务的共同训练可以减轻任务过拟合问题，使得模型对新任务和旧任务之间的知识转移更加平滑。

## PASS算法的步骤

1. **初始化模型**：

使用一个神经网络（如ResNet-18）作为特征提取器，并对初始任务的数据进行训练，计算每个类的代表性原型。

2. **记忆类原型**：

在学习完每个任务后，计算每个旧类在深度特征空间中的均值，作为该类的代表性原型，并存储这些原型。

3. **增强类原型**：

在学习新任务时，对每个旧类的代表性原型施加高斯噪声进行扰动，以生成增强的原型。扰动的强度可以根据特征的平均方差来确定。

4. **联合训练**：

将增强的旧类原型和新任务的数据一起输入分类器进行训练。使用分类损失和自监督损失的组合来优化模型参数。

为了进一步减少旧任务特征与新任务特征之间的偏差，还引入了**知识蒸馏**（Knowledge Distillation）损失，约束新旧模型的特征输出一致性。

5. **自监督任务生成**：

对当前任务的数据进行旋转等变换生成新的代理任务，将其作为额外的训练目标，增强特征的泛化能力。

6. **重复步骤2-5，处理后续任务**：

每次学习新任务时，都记忆新的类原型，并对所有旧类的原型进行增强和自监督训练。

## PASS在增量学习领域的使用和应用

PASS算法主要应用于 **类增量学习（Class-Incremental Learning, CIL）** 场景中，即每个任务由一组与之前任务不重叠的类组成。它适用于以下应用场景：

- **计算机视觉中的面部识别**：需要逐步学习新出现的面孔，而不遗忘已识别的面孔。

- **物体分类**：在工业检测或自动驾驶中，不断接收新类的物体，同时维持对旧物体的识别能力。

- **自然语言处理中的新词检测**：随着语言的演变，持续学习新词汇的表示，同时保留对旧词汇的理解。

## PASS算法的缺陷和不足

1. **高斯噪声增强的简单性**：

当前的原型增强仅使用高斯噪声，这可能不足以模拟复杂任务之间的特征转移。对于高维数据，高斯扰动可能无法捕捉到真实的类间特征变动。

2. **自监督学习任务的局限性**：

旋转变换虽然可以增强特征，但对于一些应用（如语音识别或文本分类），旋转并不是合适的自监督任务。因此，自监督学习的设计需要更有针对性。

3. **知识蒸馏的有效性受限**：

在没有存储样本的情况下，知识蒸馏的效果可能受到限制，因为模型仅通过类原型而非具体样本进行约束。

4. **对初始模型的依赖**：

如果初始任务的模型训练得不够充分，后续增量学习的表现可能会受到影响，因为模型的基础特征学习不够稳健。

## 改进建议

1. **引入更复杂的增强策略**：

考虑采用**对抗性训练**（Adversarial Training）或**自适应噪声扰动**来增强原型，使得增强后的特征能够更好地覆盖特征空间的多样性。

结合**混合增强**（如Mixup或CutMix）来进一步扩展类原型的分布。

2. **设计多样化的自监督任务**：

针对不同类型的数据（如图像、语音或文本），设计适配的自监督任务。例如，在文本分类中可以使用**词汇替换**、**句子重排**等任务，在语音识别中可以使用**时间倒转**或**频域变换**等任务。

3. **结合记忆回放策略**：

在不违反隐私或内存限制的前提下，适量存储一些旧任务的特征样本（而非原始数据），可以进一步增强模型对旧任务的记忆能力。

4. **采用动态蒸馏损失权重**：

根据当前任务和旧任务的特征差异，自适应地调整知识蒸馏损失的权重，以更好地平衡新旧任务之间的知识转移。

---

# 精读笔记

### 1. 论文的研究背景是什么？为什么增量学习（Incremental Learning, IL）是一个挑战？
**答案**：增量学习的主要挑战是**灾难性遗忘**（Catastrophic Forgetting）。在增量学习过程中，深度神经网络在学习新任务时，往往会调整先前学到的参数，从而导致遗忘之前任务的知识。论文中强调，在动态环境中，增量学习是现代人工智能的重要能力，因为现实应用中的训练样本往往是顺序出现的，例如面部识别系统会不断遇到新面孔并需要增量学习。

### 2. 论文提出的主要方法是什么？简述其核心思想。
**答案**：论文提出了一种称为**PASS（Prototype Augmentation and Self-Supervision）**的非样本存储方法。PASS的核心思想是：通过在深度特征空间中**记忆和增强类代表性原型**，并结合**自监督学习**（SSL）来学习更多可泛化的特征，以应对增量学习中的灾难性遗忘问题。

### 3. 什么是类代表性原型（Prototype）？如何进行原型增强？
**答案**：类代表性原型是指在深度特征空间中，每个类的中心点（即类的均值）。**原型增强**是指在学习新任务时，对之前记忆的原型进行高斯噪声扰动，然后将增强的原型与新数据的特征一起用于分类，从而保持旧类和新类之间的区分性和平衡性。

### 4. 为什么自监督学习（Self-Supervision, SSL）在增量学习中有效？
**答案**：自监督学习可以帮助模型学习到**任务无关的可迁移特征**。在增量学习中，任务特异性特征往往导致模型过拟合当前任务，而忽视其他任务。通过自监督学习，模型可以捕捉到更丰富、通用的特征，使得不同任务之间的参数空间更接近，减轻了过拟合问题。

### 5. 什么是灾难性遗忘？论文中的方法如何缓解这一问题？
**答案**：**灾难性遗忘**是指神经网络在学习新任务时，会遗忘之前任务的知识。PASS通过记忆类代表性原型，并在学习新任务时增强这些原型，避免特征空间的扭曲，从而保持旧任务知识的保留，减轻灾难性遗忘。

### 6. 在增量学习中，为什么数据存储有时不可行？
**答案**：存储旧数据可能会受到**内存限制或隐私问题**的限制。例如，在隐私敏感的应用场景中，存储用户的旧数据可能是不允许的。此外，存储数据会占用大量内存，尤其在数据量庞大的情况下。

### 7. 论文中提到的“过度拟合问题”指的是什么？
**答案**：过度拟合问题指的是深度神经网络在增量学习过程中，往往会过度拟合当前任务的特征，而忽视未来任务的潜在特征。这种过度拟合会使模型在学习新任务时需要更多调整，从而加剧遗忘旧任务知识的风险。

### 8. 论文中实验验证了哪些数据集？实验结果如何？
**答案**：论文的实验在**CIFAR-10**、**CIFAR-100** 和 **TinyImageNet** 等数据集上进行了验证。实验结果表明，PASS显著优于其他非样本存储方法，并且在不存储旧任务样本的前提下，与一些样本存储方法相比，表现出可比的性能。

### 9. 什么是类增量学习（Class-Incremental Learning, CIL）？
**答案**：类增量学习是一种特殊的增量学习场景，在这种场景中，每个任务包含一组与之前任务不重叠的类，模型需要学会对所有已经见过的类进行统一分类，而不能依赖任务标识符来区分当前任务。

### 10. 增量学习中的“决策边界”为什么会发生改变？
**答案**：在学习新类时，神经网络会调整参数以适应新任务的数据分布，这会改变之前类的**决策边界**，从而导致模型对旧类的区分能力下降，进而出现灾难性遗忘。

### 11. 高斯噪声在原型增强中的作用是什么？
**答案**：高斯噪声的作用是**扩展类原型的分布**，模拟旧类在特征空间中的可能变动，从而防止模型在学习新任务时对旧类原型的分类产生偏差，保持对旧类的良好区分能力。

### 12. 自监督学习中的旋转变换（Rotation Transformation）是如何应用的？
**答案**：论文通过将训练样本进行90度、180度和270度的旋转来生成新的类，并将这些变换后的样本用于自监督学习，帮助模型学习到更泛化的特征，从而提升对未来任务的适应性。

### 13. 论文中为什么没有使用生成模型（如GAN）来生成旧任务的样本？
**答案**：生成模型如GAN虽然可以生成旧任务的伪样本，但它们的训练往往**不稳定且低效**，尤其是在复杂数据集上。此外，生成模型本身也会遇到灾难性遗忘的问题。

### 14. 为什么仅使用知识蒸馏（Knowledge Distillation）无法有效解决增量学习中的遗忘问题？
**答案**：仅使用知识蒸馏会过于依赖之前模型的输出，而不考虑新的任务特征，这种方法在类增量学习中往往表现不佳，特别是在没有任务标识符的情况下，容易造成模型偏向新任务。

### 15. PASS方法与传统增量学习方法的主要区别是什么？
**答案**：PASS方法的主要区别在于它**不存储旧任务的数据**，而是通过增强类代表性原型和自监督学习来保持旧任务的知识，而传统方法通常需要存储一部分旧任务的样本或使用复杂的生成模型。

### 16. 什么是平均遗忘度（Average Forgetting），它如何衡量遗忘问题？
**答案**：**平均遗忘度**是衡量模型在增量学习过程中遗忘旧任务知识的程度。它通过比较当前任务训练后，模型对之前任务的分类准确率下降情况来计算，遗忘度越低，表示模型遗忘的旧任务知识越少。

### 17. PASS方法中的原型增强策略与长尾学习中的增强策略有什么异同？
**答案**：长尾学习中的增强策略通常通过从头类中学习嵌入增强，而PASS中的增强策略则是直接通过**高斯噪声**对原型进行扰动。两者的相同点在于都通过扩展类的分布来提升分类性能，但在目标和方法上有所不同。

### 18. 为什么在增量学习中，使用多头分类器（multi-head classifier）会有利于解决灾难性遗忘？
**答案**：使用多头分类器可以在推理时使用任务标识符来指示当前任务，这样可以避免新旧任务之间的干扰。然而，论文中提到，多头分类器在类增量学习场景下表现不佳，因为这种场景要求统一的分类器。

### 19. 论文中的方法如何应对类不平衡问题？
**答案**：PASS通过原型增强在特征空间中保持旧类的分布，并结合自监督学习减少模型对当前任务的过拟合，从而有效应对类不平衡问题，避免模型过度偏向新类。

### 20. 如何衡量模型在增量学习中的泛化能力？
**答案**：模型的泛化能力可以通过其在**新类和旧类上的分类准确率**来衡量。论文中的实验结果表明，通过自监督学习增强模型的泛化能力，可以使模型在面对未来任务时表现更好。

### 21. 论文提到的特征空间密度（Feature Space Density）是什么？它与泛化能力的关系如何？
**答案**：**特征空间密度**指的是类内距离与类间距离的比值。更高的特征空间密度意味着模型能够更好地区分不同的类，从而提高泛化能力。

### 22. 增量学习中的“稳定性-可塑性权衡”问题指的是什么？
**答案**：**稳定性-可塑性权衡**指的是模型在保持旧知识的稳定性和适应新知识的可塑性之间的平衡。如果模型过于稳定，则可能无法适应新任务；如果过于可塑，则可能遗忘旧任务。

### 23. 论文中使用了哪些评价指标来衡量方法的效果？
**答案**：主要使用了**准确率（Accuracy）**和**平均遗忘度（Average Forgetting）**等指标来衡量方法的效果。准确率衡量模型对所有已学类的分类能力，而平均遗忘度衡量模型遗忘旧任务的程度。

### 24. 论文的贡献主要体现在哪些方面？
**答案**：主要贡献包括：提出了一种非样本存储的增量学习方法PASS；强调了任务级别的过拟合现象；展示了自监督学习在增量学习中的有效性。

### 25. 为什么PASS方法更适合不存储旧数据的场景？
**答案**：因为PASS方法不依赖于旧任务样本的存储，仅通过记忆和增强类代表性原型来保持对旧任务的知识，这使得它更适用于内存有限或有隐私限制的应用场景。

### 26. 在增量学习的设置中，论文中为什么没有使用任务标识符？
**答案**：在类增量学习的场景下，模型需要学会对所有类进行统一分类，不允许使用任务标识符来区分当前任务。这样可以更好地模拟真实应用中的情况。

### 27. PASS方法中的自监督学习能否独立于原型增强单独使用？为什么？
**答案**：可以独立使用，但效果会较弱。自监督学习主要用于减少过拟合，而原型增强则用于保持旧类的特征分布，两者结合使用效果最佳。

### 28. 在实验中，为什么选择ResNet-18作为特征提取器？
**答案**：ResNet-18是一种经典的卷积神经网络结构，具有较强的特征提取能力，并且其较浅的层数使得模型更易于训练和分析。

### 29. 实验中如何选择增强的噪声尺度（Scale）？
**答案**：噪声尺度可以通过计算类特征的平均方差来确定，从而控制增强的原型的扰动程度，使其反映旧类分布的不确定性。

### 30. 自监督学习和传统的有监督学习在增量学习中的主要区别是什么？
**答案**：自监督学习通过设计代理任务（如旋转预测）来学习特征，而不是依赖于标签。这使得自监督学习可以更好地捕捉任务无关的可迁移特征，提升模型在增量学习中的泛化能力。
