# ASER - [Online Class-Incremental Continual Learning with Adversarial Shapley Value](https://aaai.org/papers/09630-online-class-incremental-continual-learning-with-adversarial-shapley-value/)

----

<img width="1153" alt="Screenshot 2024-10-21 at 2 14 06 PM" src="https://github.com/user-attachments/assets/15d7d1be-8722-45b3-9d0d-0c37350c42c9">

----

Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang. 2021. Online Class-Incremental Continual Learning with Adversarial Shapley Value. In AAAI. 9630–9638.

----

# 核心要点

论文提出了一种**基于 Shapley 值的经验重放**算法，称为 **ASER（Adversarial Shapley Value Experience Replay）**。论文的主要贡献是通过引入 Shapley 值来衡量记忆样本对模型学习的贡献，并在此基础上设计了一个对抗性的记忆重放机制，以优化模型在类增量学习场景中的表现。

## ASER 算法逻辑

ASER 算法的设计旨在通过智能选择样本来防止遗忘，同时促进模型对新任务的学习。其核心思想是通过 **Shapley 值** 来衡量每个记忆样本在保持旧任务学习效果和干扰新任务学习之间的贡献，并根据此信息来选择哪些样本需要进行记忆重放。

## 主要步骤：

1. **数据处理和记忆更新**：

- 在每个新的任务到来时，算法首先通过随机选择或特定策略从记忆缓冲区中选择一定数量的样本进行处理。

- 根据 Shapley 值计算每个样本对模型学习的贡献，确保保留对学习有利的样本，移除冗余或低效的样本。

2. **Shapley 值计算**：

- **Shapley 值** 被用来衡量每个记忆样本对模型分类边界的贡献。

- 通过最近邻（KNN）方法有效地估算每个样本的 Shapley 值，计算其对模型整体性能的平均边际贡献。

3. **记忆重放（Memory Replay）**：

- ASER 使用对抗性的 Shapley 值机制选择记忆样本：选择那些对旧类有正面贡献（即保持学习稳定性）但对新类有负面影响的样本进行重放。这些样本可以强化模型对旧任务的记忆，同时帮助模型更好地处理新任务。

4. **优化目标**：

- 通过结合旧任务和新任务样本进行训练，确保模型能够同时适应新任务，同时不丧失对旧任务的分类能力。

- **ASER** 的目标是平衡“塑性”（新任务学习）和“稳定性”（旧任务记忆）的矛盾，避免灾难性遗忘。
  
<img width="605" alt="Screenshot 2024-10-21 at 4 39 46 PM" src="https://github.com/user-attachments/assets/e9d4c5cc-ec14-490f-a888-7b5ee1471bf6">

<img width="605" alt="Screenshot 2024-10-21 at 4 40 11 PM" src="https://github.com/user-attachments/assets/6dbaff0b-e0d0-478c-8c61-979982e7a814">

### ASER 的步骤总结：

1. **初始化记忆缓冲区**，开始新任务。

2. **Shapley 值计算**：通过 KNN 近邻计算每个样本对模型的边际贡献。

3. **对抗性重放**：选择对新类有干扰（负面贡献）、对旧类有保护作用（正面贡献）的样本进行记忆重放。

4. **更新记忆缓冲区**：根据新的 Shapley 值重新调整记忆中的样本，替换不重要或无用的样本。

5. **训练和优化**：结合新的任务数据和挑选出的记忆样本进行训练，更新模型。

### ASER 在 Incremental Learning 领域的应用

ASER 算法专门为 **类增量学习（Class-Incremental Learning）** 设计，该领域的核心任务是要求模型能够持续学习新类，而不丧失对旧类的记忆。ASER 通过智能选择样本进行记忆重放，帮助模型在逐步扩展的任务中保留以前学习的知识，并能够很好地适应新任务。

## ASER 的应用场景：

1. **设备端持续学习**：在移动设备或物联网设备中，由于计算和存储资源有限，模型需要从连续的数据流中学习，同时不能忘记先前学到的知识。

2. **数据隐私保护**：通过局部存储数据并在本地更新模型，ASER 有助于在数据隐私保护的场景中持续优化模型，而无需上传数据到云端。

3. **动态变化的分类任务**：在金融、医疗或其他应用中，分类任务的类别会不断变化，ASER 可以帮助模型在此类动态变化的环境中保持稳定的性能。

### ASER 的缺陷和不足

尽管 ASER 提供了创新的解决方案，但它也存在一些不足之处：

1. **计算复杂度高**：Shapley 值的计算虽然在理论上很公平，但其计算代价较高，尤其是在大规模数据集和多类分类任务中。即使使用 KNN 来加速计算，仍然可能会对大数据场景带来性能瓶颈。
   
2. **对抗性选择可能导致样本不均衡**：ASER 强调对抗性样本选择，可能会导致过于集中于某些关键样本，忽视其他潜在重要的样本，从而导致样本选择的多样性不足，影响模型泛化能力。

3. **内存依赖问题**：虽然 ASER 试图通过优化记忆重放来减少遗忘，但其依赖于有限的记忆缓冲区大小。当记忆容量过小时，效果会明显下降。

### 改进建议

1. **优化 Shapley 值计算**：可以考虑使用更轻量级的近似方法来估算 Shapley 值。例如，基于随机化的近似方法，或在不影响公平性的前提下引入剪枝策略，减少不必要的计算。

2. **引入多样性约束**：在样本选择中加入多样性约束，避免过度依赖对抗性选择。可以设计一种机制，使得在确保对抗性的同时，也能够选取更多元的样本来保持模型的泛化能力。

3. **动态记忆调整机制**：根据任务的复杂度和数据的特性，动态调整记忆缓冲区的大小。对于重要的新任务，分配更多的记忆资源，而对于较简单的任务，减少记忆消耗。

4. **多模型融合**：引入多模型集成或知识蒸馏的方法，使得在学习新任务时，可以保留部分旧模型的知识，以进一步减少遗忘风险。例如，可以使用蒸馏损失将旧模型的输出与新模型的输出进行对比，保留重要的历史信息。

-----

# 精读笔记

### 1. 什么是 **在线类增量持续学习（Online Class-Incremental Continual Learning）**？

**答案**：这是指模型在面对连续的数据流时，能够逐步学习新类，而不会忘记已学过的类。

**解释**：模型在处理不断到来的新任务（新类）时，不仅要学会新类，还要保持对旧类的记忆。

### 2. 论文中提到的主要挑战是什么？

**答案**：主要挑战是 **灾难性遗忘**，即模型在学习新任务时，容易忘记以前学过的任务。

**解释**：就像学新东西时，可能会忘记以前的内容，这也是深度学习模型的一大难题。

### 3. 什么是 **Adversarial Shapley Value Experience Replay (ASER)** 方法？
**答案**：ASER 是一种基于 Shapley 值的经验重放方法，用于选择最有贡献的数据进行记忆重放，以减少遗忘并提高新任务的学习能力。
**解释**：这个方法像是在打分，给每个旧任务的数据分配“贡献值”，帮助模型回忆之前学过的重要内容。

### 4. 什么是 **Shapley 值**，它在这篇论文中的作用是什么？
**答案**：Shapley 值来源于合作博弈论，用于衡量每个参与者对整体贡献的公平分配。在论文中，它用于衡量每个记忆样本对模型学习的贡献。
**解释**：可以把它想象成是一个“公平的评分系统”，评估每个数据点对学习的影响。

### 5. **ASER** 如何平衡稳定性与灵活性？
**答案**：通过对记忆中的样本和当前任务样本进行对抗性选择，ASER 保持记忆稳定性（不遗忘旧任务），同时促进对新任务的学习。
**解释**：ASER 会找出对模型稳定性最重要的数据，同时也会选取可能扰乱模型的样本，帮助模型保持灵活学习。

### 6. 什么是 **灾难性遗忘（Catastrophic Forgetting）**？
**答案**：这是指神经网络在学习新任务时，忘记了以前学过的任务。
**解释**：就像我们学习新知识时，可能会忘记之前的内容，深度学习模型也有类似问题。

### 7. **持续学习** 和 **传统学习** 有何不同？
**答案**：持续学习需要模型在面对不断变化的数据流时，不仅要学会新内容，还不能忘记旧知识。传统学习通常只需要在一次性训练中学习所有内容。
**解释**：持续学习更像是我们日常中的“终身学习”，而传统学习更像是一次考试复习。

### 8. 为什么持续学习在实际应用中很重要？
**答案**：在许多现实场景中，如手机和可穿戴设备上，数据是连续的，持续学习可以让模型适应不断变化的环境。
**解释**：就像我们的手机应用会持续获取新信息，模型也需要不断适应新数据。

### 9. **记忆重放（Memory Replay）** 的概念是什么？
**答案**：记忆重放是一种通过在训练过程中反复使用之前见过的数据样本来防止遗忘的技术。
**解释**：就像我们复习旧知识以防遗忘，模型也可以通过重放旧数据来保持记忆。

### 10. **MIR（Maximally Interfered Retrieval）** 是什么？
**答案**：MIR 是一种经验重放方法，它选择那些在学习新任务时损失最大的样本进行重放，以最大化对抗干扰。
**解释**：MIR 会优先选择那些可能被新任务破坏的旧任务样本来帮助模型保持记忆。

### 11. 论文中使用的模型评估标准有哪些？
**答案**：论文使用了 **平均准确率** 和 **平均遗忘率** 来评估模型表现。
**解释**：平均准确率衡量模型整体表现，而平均遗忘率衡量模型对旧任务的遗忘程度。

### 12. **单头评估（Single-Head Evaluation）** 是什么？
**答案**：单头评估是指在测试时，模型不使用任务身份，必须在所有类中进行分类。
**解释**：这就像我们不知道问题属于哪一类时，必须在所有可能的答案中进行选择。

### 13. **ER（Experience Replay）** 与 **ASER** 的区别是什么？
**答案**：ER 采用随机采样的方式来选择记忆样本，而 ASER 通过 Shapley 值来选择对模型学习贡献最大的样本。
**解释**：ER 类似于随机复习旧知识，而 ASER 更像是有策略地复习最重要的内容。

### 14. **KNN Shapley 值** 是如何计算的？
**答案**：KNN Shapley 值通过计算每个样本对其最近邻分类器（KNN）的贡献来衡量其价值。
**解释**：这就像看一个数据点在其周围的邻居中有多大影响，从而评估其重要性。

### 15. 为什么选择 Shapley 值用于数据选择？
**答案**：Shapley 值具备公平性和合理性，能够公平地评估每个样本对整体模型表现的贡献。
**解释**：Shapley 值保证了每个样本的贡献被公平考虑，避免某些样本被过度或不足重视。

### 16. 论文中的 **ASERμ** 与 **ASER** 有何不同？
**答案**：ASERμ 使用的是 Shapley 值的平均版本，能够避免因单个异常点导致的不稳定结果。
**解释**：ASERμ 更加平滑和稳定，因为它考虑了整体的贡献，而不是个别的极端数据点。

### 17. **存储记忆（Memory Update）** 在 ASER 中如何实现？
**答案**：ASER 根据 KNN Shapley 值的大小，替换记忆中贡献较小的样本。
**解释**：它像是每次都用对模型最有帮助的新数据来替换那些不太重要的旧数据。

### 18. 论文中的实验基准数据集有哪些？
**答案**：包括 **Split CIFAR-10**、**Split CIFAR-100** 和 **Split miniImageNet**。
**解释**：这些数据集通过划分不同任务来测试模型在持续学习中的表现。

### 19. 论文中的 **MIR** 方法存在什么问题？
**答案**：MIR 在选择重放样本时，容易导致选择过于相似的样本，增加数据冗余。
**解释**：就像复习时总是看同一类题目，这可能会导致对其他知识点的遗忘。

### 20. **平均准确率** 的公式是什么？
**答案**：平均准确率是指模型在所有任务上的平均测试准确率。
**解释**：它衡量模型在所有学习任务中的整体表现。

### 21. **平均遗忘率** 的公式是什么？
**答案**：平均遗忘率衡量的是模型在完成新任务后，对旧任务的遗忘程度。
**解释**：它像是看模型是否在学新内容的同时忘记了旧的知识。

### 22. **参数隔离（Parameter Isolation）** 方法如何工作？
**答案**：通过为每个任务分配独立的参数，防止不同任务之间的干扰。
**解释**：像是为每个任务单独开辟一个区域，互不干扰。

### 23. **正则化方法（Regularization Methods）** 如何防止遗忘？
**答案**：通过在损失函数中加入惩罚项，限制模型更新时对关键参数的改变。
**解释**：类似于为模型设置一个“保护机制”，避免其轻易修改重要的部分。

### 24. 为什么 **ASER** 在记忆较小时效果更好？
**答案**：因为 ASER 能够有效选择最重要的记忆样本，最大化记忆的利用率。
**解释**：当记忆空间有限时，选择对模型最有帮助的数据变得更加重要。

### 25. **全局数据评估** 在 ASER 中的重要性是什么？
**答案**：ASER 能够从全局角度评估数据的重要性，而不仅仅基于单个任务的损失变化。
**解释**：它不像只关注局部错误，而是从整体模型表现的角度来选择数据。

### 26. **在线学习** 与 **批量学习** 有什么不同？
**答案**：在线学习逐个接收数据进行学习，而批量学习一次处理所有数据。
**解释**：在线学习更像是不断积累新知识，而批量学习则是一次性消化所有内容。

### 27. **参数隔离方法** 为什么不适合本文提出的场景？
**答案**：因为它需要任务身份来选择相应的参数，而在单头评估中没有任务身份。
**解释**：单头评估要求模型能够直接处理所有类，而不是依赖任务标签。

### 28. 论文提出的 **ASER** 与 **传统经验重放** 方法相比有何优势？
**答案**：ASER 能够更智能地选择对当前学习和旧任务保护都至关重要的样本，而传统经验重放往往是随机选择。
**解释**：ASER 就像是有策略地进行“复习”，而传统方法更像是随机选择内容进行回顾。

### 29. **KNN-SV** 如何帮助模型在嵌入空间中选择数据？
**答案**：通过计算每个样本在最近邻分类器中的贡献值，选择那些对分类边界最有用的数据。
**解释**：它帮助模型识别哪些样本最能保护当前的分类能力。

### 30. **未来工作** 可以在哪些方面进一步改进 ASER？

**答案**：可以尝试更加复杂的样本利用方法，或开发特定于持续学习的效用函数。

**解释**：可以研究如何更高效地使用重放样本，或者为 Shapley 值设计更适合持续学习的评价标准。












