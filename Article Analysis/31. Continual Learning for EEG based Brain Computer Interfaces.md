
# [Continual Learning for EEG based Brain Computer Interfaces](https://openreview.net/forum?id=9Y_wci2OC3)

----

<img width="1366" alt="Screenshot 2025-04-02 at 12 51 42 AM" src="https://github.com/user-attachments/assets/d7660f15-5ddb-4385-89c0-176a101bae69" />

-----

### Domain-incremental learning

----

### 研究背景

脑机接口 （BCI） 系统中引入持续学习是必要的和机会。我们进行了实证评估，以了解 CL 策略对基准数据集的影响。我们在学科内和跨学科场景中的发现表明，CL 策略可以胜过离线学习，并为 BCI 应用程序构建稳健的模型。在跨学科场景中，当将知识从一个学科转移到另一个学科时，CL 可能导致学习不变的学科表示。在主题内方案中，CL 可以在将知识从一个会话传输到另一个会话时提高性能。

###  实验方法

1. **重播** Replay：为下一次体验保留部分前一个数据集。

2. **弹性加权整合** Elastic Weighted Consolidation (EWC)：根据模型权重的重要性值限制模型权重的变化。

3. **不忘记学习** Learning without Forgetting (LwF)：使用知识蒸馏和迁移学习的组合。

4. **情景记忆梯度** Gradient of Episodic Memory (GEM)：通过使用以前经验的模式的外部情景记忆，将梯度投射到当前小批量上。

横向方法： **朴素迁移学习和累积策略**（包括当前和所有以前的经验作为训练集）naive transfer learning and cumulative strategy

### 结论

1） CL 策略的表现优于离线

在大多数情况下，除了三种情况外，CL 策略的表现都优于离线学习。而对于主题 ID 8（主题内场景），离线学习优于 BCI，这表明 CL 在 BCI 应用程序中有相当大的空间。

2） 内存驱动 vs. 正则化策

对于 **跨主体 cross-subject** 和 **主体内 within-subject** 场景，记忆或重放驱动的策略，即**重放**和**情景记忆梯度 （GEM）**，优于其他策略。原因可能与发现在这种情况下没有灾难性的遗忘有关，而其他方法则侧重于通过正则化来避免遗忘。

3） CL 的停止或控制标准

可以注意到在对特定体验进行培训后，准确性显着下降。为 CL 策略定义控制策略或停止标准至关重要，以便通过学习新数据来保护模型免受恶化。

![image](https://github.com/user-attachments/assets/74045df6-53d1-415e-a1ff-0abe1291ff27)

