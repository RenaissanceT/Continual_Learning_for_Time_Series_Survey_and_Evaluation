
# [Learning Fast and Slow for Online Time Series Forecasting](https://arxiv.org/abs/2202.11672)

----

<img width="1366" alt="Screenshot 2025-04-02 at 12 57 52 AM" src="https://github.com/user-attachments/assets/fb593fc1-5936-4cc9-906b-daed785c88ef" />

-----

### **领域** : 

属于增量学习，针对在线时间序列预测问题

### **分类** : 

属于增量学习中的**域增量学习**，因为研究了如何在非平稳环境中进行持续学习

### **多任务** : 

涉及了多个数据集，并在不同时间阶段进行预测任务

### **场景** : 

设定场景是**在线时间序列预测**，目标是通过增量学习方法提高对非平稳数据流的预测能力。属于online learning，因为模型需要随着数据的变化进行动态调整

### **问题** : 

研究问题是如何在非平稳数据流中进行稳定学习，同时减少遗忘

域的划分基于**数据流的概率分布变化**，不同时间阶段的数据分布不同，导致模型需要适应多个数据分布

![image](https://github.com/user-attachments/assets/95df7f74-4fc1-4ad6-a61a-949336aaa47a)

Task 划分是隐式的，基于时间的连续性与环境的非平稳性 non-stationarity 假设

> 认为时间序列本身具有非平稳性，即数据分布随时间发生变化，因此从 online learning 的角度，每个新分布就可以看作一个新的“任务”

> 模型不显式标记“这是第几个任务”，但通过数据的 分布漂移 distributional shift 来触发快学习器和慢学习器的不同权重更新

### **方法** : 

提出了一种**Fast and Slow learning Networks（FSNet）**，通过**适配器（adapter）** 和 **关联记忆（associative memory）** 来优化模型适应性

属于**预测**，目的是提高时间序列预测的准确性。具体使用 **Replay Methods**，通过存储过去的数据并在后续训练中重用，以减少遗忘并提高模型的稳定性

> 快速学习器：基于最近的数据快速适应（比如使用 Online Gradient Descent）

> 慢速学习器：基于长期累积的数据，保持对整体环境的稳定适应

> 如果近期数据与长期趋势差异大（即产生了任务/域漂移），那么快学习器的影响更大，从而让模型适应新“任务”

### **为什么** : 

采用FSNet框架的原因是通过适配器和关联记忆，该方法能够更有效地适应数据流的变化，提高时间序列预测的长期稳定性

----


## 研究背景

深度神经网络在时间序列预测中取得成功，但多基于 **批量训练（batch learning）**，假设数据静态。实际应用中数据通常是流式的，并伴随 **概念漂移（concept drift）**。

在线训练存在两大挑战：

- 对新数据适应慢（学习效率低）；

- **灾难性遗忘**，无法有效记住或利用过去的模式。

新框架 **FSNet (Fast and Slow learning Network)**，启发来自 **神经科学中的 CLS 理论（Complementary Learning Systems）**。是一种面向 **非平稳环境下时间序列预测任务**的连续学习框架，通过模拟人脑“快与慢”的互补机制，实现对新概念的快速适应与对旧知识的有效记忆。

FSNet 包括两个互补模块：

1. **Per-layer Adapter**：每一层的轻量化快速适应器，支持快速局部学习

2. **Associative Memory**：关联记忆模块，用于存储和回忆重复出现的历史适应行为

## 算法

### 1. 架构

![image](https://github.com/user-attachments/assets/a81b54fe-2fb4-48ed-b910-03b034a70fef)

（FSNet 中的每个卷积滤波器都配备了一个适配器和关联存储器，通过监控主干的梯度 EMA 来促进快速适应新旧模式。）

主干网络为 **Temporal Convolutional Network (TCN)**， 在每一层加上：**适配器 φ<sub>l</sub>** + **记忆模块 M<sub>l</sub>**；

总参数集为 ω = {θ<sub>l</sub>, φ<sub>l</sub>}<sup>L</sup>，总记忆集为 M = {M<sub>l</sub>}<sup>L</sup>。

<img width="903" alt="Screenshot 2025-05-21 at 10 48 28 AM" src="https://github.com/user-attachments/assets/576ffafb-bdde-45f2-938d-150a600fece9" />

### 2. 快速适应机制（Fast Adaptation）

使用滑动平均（EMA）平滑每层梯度：  

![image](https://github.com/user-attachments/assets/71ff4113-1926-4e4a-8d69-8361b410d162)

生成适配系数：

权重适配：α<sub>l</sub>；
特征适配：β<sub>l</sub>；

应用方式：

![image](https://github.com/user-attachments/assets/fd7777ef-8cb5-476e-a2c9-42fa30f8691b)

### 3. 重复模式记忆机制（Associative Memory）

存储学习过程中遇到的重复事件的适应系数。因此，单独的适配器可以处理短时间尺度上的快速变化，而联想存储器可以促进重复模式的学习

存储适配向量 u（= [α<sub>l</sub>, β<sub>l</sub>]）以记忆过去应对某一模式的行为

当**触发条件**满足：

![image](https://github.com/user-attachments/assets/77cb54a1-2a15-4361-8623-b023ef4c0cdd)

- 从 M<sub>l</sub> 中**读取最相似记忆**；

适配器-内存交互机制 由于当前的适应系数可能无法捕获整个事件，这可能跨越几个样本，因此我们 使用 adaptation coefficients 的 EMA 执行内存读写作，以完全捕获当前 pattern

- 当前适配器参数与回忆融合：  

![image](https://github.com/user-attachments/assets/fc062fd1-4e88-4c33-8d52-f86979077482)

- 用 outer product 写入更新：  

![image](https://github.com/user-attachments/assets/4e4cd487-e90e-40b7-af7b-4e1726f946b0)

## FSNet 应用

**时间序列在线预测**被形式化为 **无任务边界（task-free）连续学习问题**

1. 每段局部平稳序列视为一个“任务”，FSNet可处理新旧任务无缝切换

2. **不依赖任务切换标记点**

3. 强调对当前样本的利用和过去知识的回忆。

应用场景： 
- 电力负荷预测（ECL 数据集）；
- 气象建模（WTH 数据集）；
- 交通预测（Traffic）；
- 高频交易信号等实时智能系统。

## 结论

我们研究了在非平稳环境中训练深度神经网络进行在线时间序列预测的局限性，在这些环境中，它们缺乏快速适应新的或重复出现的模式的能力。然后，我们通过将用于持续学习的 CLS 理论扩展到在线时间序列预测来提出快速和慢速学习网络 （FSNet）。FSNet 通过两个关键组件增强了神经网络主干：（i） 用于适应最近变化的适配器;（ii） 用于处理循环模式的联想记忆。此外，适配器与其内存稀疏交互以存储、更新和检索重要的重复模式，以便将来学习此类事件。广泛的实验表明，FSNet 能够处理各种类型的概念漂移，从而在真实世界和合成时间序列数据中取得有希望的结果。


