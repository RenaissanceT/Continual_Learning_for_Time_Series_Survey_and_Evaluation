# MAS - [Memory Aware Synapses: Learning what (not) to forget](https://dl.acm.org/doi/10.1007/978-3-030-01219-9_9)

----

<img width="1366" alt="Screen Shot 2024-10-15 at 10 33 17 AM" src="https://github.com/user-attachments/assets/16f65b5e-8335-4a07-89a2-df3aa4b916e5">

----

Rahaf Aljundi,Francesca Babiloni,Mohamed Elhoseiny,Marcus Rohrbach,and Tinne Tuytelaars. 2018. Memory aware synapses: Learning what (not) to forget. In ECCV. 139–154.

---

# 核心要点

----






----

# 精读笔记

### 1. **什么是终身学习（Lifelong Learning, LLL）？为什么它在机器学习中很重要？**
   - **答案**：终身学习指的是一个模型能够在接收新的任务时，不断学习新的知识，同时保持对以前学习任务的记忆。这在机器学习中非常重要，因为数据不断变化，模型需要不断适应新的环境而不会丢失已有的知识。
   - **解释**：比如一个图像分类模型最初学会了识别猫狗，但之后还需要学习识别新物种。在这个过程中，模型要避免“灾难性遗忘”（即忘记如何识别猫狗），这就是终身学习的应用场景。

### 2. **论文中提到的“灾难性遗忘”是什么？**
   - **答案**：“灾难性遗忘”是指当神经网络在学习新任务时，模型的参数更新导致其忘记之前学到的知识的现象。
   - **解释**：这就像我们学习了数学，但当开始学化学后，如果数学知识被“覆盖”，我们就无法很好地解决科学问题。

### 3. **论文中提出了哪种方法来避免灾难性遗忘？**
   - **答案**：论文提出了“记忆感知突触”（Memory Aware Synapses，MAS）方法，通过计算网络中每个参数的重要性，并在训练新任务时避免修改重要的参数。
   - **解释**：MAS 模仿了生物神经网络的学习方式，给重要的突触分配较高的权重，从而保护它们不被轻易修改。

### 4. **MAS与Hebbian学习法的关系是什么？**
   - **答案**：MAS 的局部版本（local MAS）与Hebbian学习法相关。Hebbian法强调“同时激活的神经元会加强连接”，而MAS中的参数重要性也是基于类似的激活模式。
   - **解释**：Hebbian法类似于“用进废退”，如果两种输入多次共同出现，它们的连接会加强。

### 5. **MAS计算参数重要性时是如何做到无监督的？**
   - **答案**：MAS通过测量输出函数对参数变化的敏感度来计算重要性，而不依赖于标注数据。它只需通过无标签的样本更新权重即可。
   - **解释**：这意味着MAS可以在实际环境中测试时动态调整，而无需依赖人工标注的数据。

### 6. **Elastic Weight Consolidation (EWC) 与 MAS 方法的主要区别是什么？**
   - **答案**：EWC基于Fisher信息矩阵来计算参数的重要性，而MAS是基于输出函数对参数变化的敏感度进行计算。MAS不需要标签数据，且可以在线更新。
   - **解释**：EWC 需要大量的计算资源来估计Fisher矩阵，而MAS相对简单且灵活。

### 7. **论文中的“重要性权重”是如何计算的？**
   - **答案**：通过计算输出函数对参数的梯度大小，MAS累积每个参数的梯度大小来表示其重要性。重要性权重越大，说明该参数对模型预测的影响越大。
   - **解释**：类似于在神经网络中，某些连接对预测结果的影响很大，因此我们不希望它们在学习新任务时被改变。

### 8. **MAS如何处理无标签的数据？**
   - **答案**：MAS不依赖标签，它通过输入数据和模型的预测结果来计算梯度并更新参数重要性。因此，它可以在无监督的条件下运行。
   - **解释**：这使得MAS能适应实际场景，比如在没有标签的测试数据中仍能动态调整。

### 9. **在终身学习的背景下，为什么需要有“固定的记忆容量”？**
   - **答案**：因为在现实世界中，模型的存储资源是有限的，不能无限增加参数或存储所有数据。因此，模型必须在固定的资源下有效地学习并保存重要的知识。
   - **解释**：这类似于人类的大脑，虽然存储能力有限，但我们会根据经验选择性地记住重要的信息。

### 10. **MAS的局部版本（local MAS）相比于全局版本有什么优势和劣势？**
   - **答案**：局部版本计算效率较高，因为它只考虑每一层的局部输出，而不计算整个网络的输出。但相对来说，局部版本的精度较低，容易忽略全局信息。
   - **解释**：就像只关注一个团队中的个人表现，而忽略整个团队的合作效率，局部版本的计算更快但视角更狭隘。

### 11. **论文中使用了哪些任务来验证MAS的效果？**
   - **答案**：论文通过物体识别任务和基于图像的<主体，谓语，宾语>三元组学习任务来验证MAS的效果。
   - **解释**：这些任务测试了模型在连续学习新任务时保持之前知识的能力，以及它如何在无标签数据中动态调整。

### 12. **为什么作者选择不依赖损失函数来计算参数重要性？**
   - **答案**：损失函数有时会陷入局部最小值，导致梯度接近于零，从而难以计算参数的重要性。而输出函数的敏感度不受这些局部最小值的影响。
   - **解释**：就像如果我们在一个错误的策略上陷入困境，依赖错误的反馈将无法改进我们的策略，输出函数敏感度则提供了一个更直接的评价指标。

### 13. **什么是多任务学习中的“前向传递”（forward transfer）？**
   - **答案**：前向传递是指在学习新任务时，模型能够利用从之前任务中学到的知识来提高新任务的学习效率。
   - **解释**：就像我们学过英语后，再学习西班牙语时可以利用相似的语法规则，这就是前向传递。

### 14. **终身学习中的“过度遗忘”与“灾难性遗忘”的区别是什么？**
   - **答案**：过度遗忘指的是模型在学习新任务时非必要地忘记了大量的旧任务信息，而灾难性遗忘则是模型完全丧失了之前任务的知识。
   - **解释**：过度遗忘像是我们可能忘记了一些不常用的知识，而灾难性遗忘则像是完全忘记了如何走路。

### 15. **在实验结果中，MAS方法在面对两类任务时与其他方法相比表现如何？**
   - **答案**：在实验中，MAS方法在保留旧任务知识的同时，新任务的学习性能几乎不受到影响，相比其他方法具有更低的遗忘率。
   - **解释**：MAS避免了“灾难性遗忘”，其效果优于像EWC和SI等方法。

### 16. **MAS方法中的正则项（Regularizer）如何帮助防止忘记？**
   - **答案**：正则项通过对重要参数的变化施加惩罚，防止这些参数在学习新任务时发生显著变化。
   - **解释**：就像在改进一栋建筑时，我们不会轻易改变它的地基结构。

### 17. **MAS如何动态适应测试条件？**
   - **答案**：MAS能够根据测试时的数据重新计算参数的重要性，并在无监督的情况下调整模型，使其适应新的测试条件。
   - **解释**：这就像一个机器人在不同环境中工作时，它能够根据实际情况重新校准其工作策略。

### 18. **论文中提到的<主体, 谓语, 宾语>三元组学习任务的挑战是什么？**
   - **答案**：挑战在于所有任务都共享网络的所有层，这意味着每层的参数都可能同时影响多个任务，从而使得学习新任务时防止遗忘更加困难。
   - **解释**：就像如果我们用一个工具来完成多个不同的任务，每次使用工具时都要避免损坏它对其他任务的适用性。

### 19. **MAS是否能够在无监督的情况下估计参数的重要性？为什么？**
   - **答案**：是的，MAS可以通过无标签数据来估计参数的重要性，因为它不依赖损失函数，而是基于输出函数的敏感性。
   - **解释**：这使得MAS在实际应用中更具适应性，可以使用未标注的数据动态调整模型。

### 20. **如何评价MAS在面对长任务序列时的表现？**
   - **答案**：在面对长任务序列时，MAS仍然保持了较低的遗忘率，并且在处理每个任务时表现稳定。
   - **解释**：这表明MAS方法在处理复杂和长时间序列任务时仍能有效地保留旧知识，同时学习新任务。

### 21. **MAS的正则化参数λ对模型性能有什么影响？**
   - **答案**：λ值越大，对参数变化的惩罚越重，模型倾向于保留更多旧任务的知识，但可能会限制新任务的学习。λ值越小，模型则更倾向于学习新任务，但可能遗忘更多旧知识。
   - **解释**：这类似于在一个系统中平衡“保守”与“创新”的程度。

### 22. **为什么在一些实验中，MAS局部版本的表现不如全局版本？**
   - **答案**：局部版本只考虑每层的局部信息，可能无法捕捉到跨层的全局信息，因此在复杂任务上表现不如全局版本。
   - **解释**：就像如果我们只关注每个团队成员的表现，而不考虑团队整体的协作效率，结果可能不如整体考量来得好。

### 23. **在实验中，MAS对那些数据稀疏的区域是如何处理的？**
   - **答案**：MAS会降低这些区域相关参数的重要性，这使得这些参数可以被用来学习其他任务，而不会影响稠密区域的输出。
   - **解释**：这类似于当我们发现某个知识点很少被使用时，可以选择性地将其遗忘。

### 24. **与Synaptic Intelligence (SI)相比，MAS的主要优点是什么？**
   - **答案**：MAS无需依赖损失函数进行参数重要性计算，更加灵活且计算效率更高。同时，MAS能够根据无标签数据动态调整模型。
   - **解释**：MAS相比SI更适用于动态变化的实际环境。

### 25. **为什么说MAS是一种模型基于参数而不是基于数据的方法？**
   - **答案**：MAS不直接依赖数据间的分布或任务标签，而是通过估计网络参数的敏感度来确定其重要性。
   - **解释**：这使得MAS可以在没有标签或新数据分布与之前数据分布差异较大的情况下仍然有效。

### 26. **如何在MAS中决定某个参数是否可以被改变？**
   - **答案**：通过重要性权重，参数权重越大，表示其对之前任务的影响越大，因此不应轻易改变。权重较小的参数则可以用于学习新任务。
   - **解释**：类似于一个关键的机械零件不应轻易调整，而次要零件则可以灵活替换。

### 27. **MAS在没有标签的数据集上执行时，如何更新参数重要性？**
   - **答案**：MAS通过输入无标签数据，并计算模型预测对参数的敏感度，更新参数的重要性。重要性越大，参数越不能被改变。
   - **解释**：这是一个动态过程，即使没有标签，也可以根据数据的频率和影响来更新模型。

### 28. **为什么固定的记忆容量对MAS方法至关重要？**
   - **答案**：因为模型的参数量有限，不可能无限存储所有任务的信息，固定记忆容量意味着模型必须有选择地保存重要信息。
   - **解释**：这类似于我们人类在有限的脑容量下，优先记住对未来有用的信息。

### 29. **为什么“Lifelong Learning”通常不储存过往任务的数据？**
   - **答案**：存储所有过往任务的数据可能会带来巨大的存储开销，且在隐私、计算资源等方面也存在实际限制。
   - **解释**：想象如果我们需要记住所有生活细节，那么大脑将会被信息淹没，因此需要选择性记忆。

### 30. **论文的主要贡献是什么？**
   - **答案**：论文的贡献在于提出了MAS方法，能够无监督、在线地估计参数重要性，并有效避免灾难性遗忘。同时，MAS展示了与Hebbian学习法的连接，适用于多任务学习场景。
   - **解释**：MAS方法为终身学习提供了新的解决方案，使得模型能够灵活适应不断变化的任务环境。

